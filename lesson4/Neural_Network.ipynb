{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先过一下理论，随后整理代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What does a neuron compute?\n",
    "Within an artificial neural network, a neuron is a mathematical function that model the functioning of a biological neuron. Typically, a neuron compute the weighted average of its input, and this sum is passed through a nonlinear function, often called activation function.\n",
    "\n",
    "A hidden layer maps input features to  new features,which are computed by neurons that one neuron output/represent one new feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2. Why we use non-linear activation funcitons in neural networks?\n",
    "Non-linear functions address the problems of a linear activation function: They allow backpropagation because they have a derivative function which is related to the inputs. They allow “stacking” of multiple layers of neurons to create a deep neural network.\n",
    "\n",
    "Without non-linear activation funciton,no matter how deep the network is,it's the same as one layer on account of “stacking”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the 'Logistic Loss' ?\n",
    "\n",
    " The loss function of logistic regression is doing this exactly which is called Logistic Loss.\n",
    " \n",
    "$$L(\\hat{y} ,y) = -y log(\\hat{y})-(1-y)log(1-\\hat{y})$$\n",
    "$\\hat{y}$ is the predicted value and y is the label value.\n",
    "Intuitively, we want to assign more punishment when predicting 1 while the actual is 0 and when predict 0 while the actual is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Assume that you are building a binary classifier for detecting if an image containing cats, which activation functions would you recommen using for the output layer ?\n",
    "\n",
    "A. ReLU    \n",
    "\n",
    "B. Leaky ReLU  \n",
    "\n",
    "C. sigmoid         ✔ \n",
    "\n",
    "D. tanh  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Why we don't use zero initialization for all parameters ?\n",
    "If you initialize all weights with zeros,then every hidden unit will get zero independent of the input. So, when all the hidden neurons start with the zero weights, then all of them will follow the same gradient and for this reason \"it affects only the scale of the weight vector, not the direction\".\n",
    "\n",
    "Initializing all weights with a same value doesn't work,either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you implement the softmax function using python ? \n",
    "Sure,let's go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 先看下我们使用到的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1797个样本，每个样本8x8的像素\n",
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#这里存的是样本的label\n",
    "digits.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAADSCAYAAAB0FBqGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAPcUlEQVR4nO3df2xW93XH8c+ZKSlJFuMAixbIMFEmVKsRP2Jl2SIFaMmUdlNhkxK1UiuIJoGmbAI0abC/Qv4DaZrgj2liShZbWpcK0haqaepKFJu10pbNDmZNSlEJmAJpfiCCm23R0rCzP+xIZPL3XD/X9nMu8/sloUDO8/gef7n3k8vDyfeauwsA0H6/lN0AAMxVBDAAJCGAASAJAQwASQhgAEgyr5UXL1682Lu7u1s+yHvvvRfWL126VKzdcccdxdqyZcuKtY6OjurGJjE6OqorV67YVF9fd02qnDlzpli7fv16sXb33XcXawsXLqzdz/Dw8BV3XzKV187Wmrz//vvF2htvvFGsLViwoFhbuXJl7X5aWROp/rq89dZbYf3y5cvF2vz584u1np6eYu1mv36ia+T8+fPF2n333TfjvUjlc6WlAO7u7tbQ0FDLBz9y5EhY3717d7H26KOPFmv79u0r1rq6uqobm0Rvb29Lr6+7JlXWr19frF27dq1Ye+aZZ4q1TZs21e7HzC5M9bWztSaDg4PF2ubNm4u11atX1/qaVVpZE6n+uuzfvz+s79mzp1hbunRpsfbyyy8Xazf79RNdI1u3bi3Wjh49OuO9SOVzhY8gACAJAQwASQhgAEhCAANAEgIYAJK0NAVRVzTlIMVjIdEI25133lmsHT58ODzm448/HtazRSNjJ06cKNYGBgaKtelMQbTDyMhIWN+wYUOx1tnZWayNjo7WbaltokmGqnP50KFDxdr27duLteHh4WJt48aN4TGbrq+vr1iLpmLajTtgAEhCAANAEgIYAJIQwACQhAAGgCQEMAAkmbExtGikJRozk+KdrO69995iLdqoJ+pHyh9Dqxq5qrtJTJNGbFpVtRHKqlWrirVoM55og6Km2LZtW7FWNcb5wAMPFGsrVqwo1m7mUbNosx0pHkPbuXNnsTadkcU6u7pxBwwASQhgAEhCAANAEgIYAJIQwACQhAAGgCQEMAAkmbE54GjbyLVr14bvjWZ9I9H8YxMcOHCgWNu7d2/43rGxsVrHjB7m2XTRfKYUz1lG7236NpxSfA2cO3cufG80Zx/N+kbXbN2HcrZLNOcrxfO80UM5o/Oo6qniVdf0ZLgDBoAkBDAAJCGAASAJAQwASQhgAEhCAANAkraMoUXbRs7WMZswRhONtESjMFL9/qu26csW9ReN7UnV21WWVI0sNV3VmObVq1eLtWgMLaq99NJL4THbcX0dO3asWNu1a1f43i1bttQ65sGDB4u1559/vtbXjHAHDABJCGAASEIAA0ASAhgAkhDAAJCEAAaAJDM2hhaNpVQ9oTgSjZoNDQ0Va0888UTtY97MoqctN+GJydGOUdEIUJVoRK1qF6ubXXTtReNk27dvL9b2798fHnPfvn3VjU1TZ2dnrZok9ff3F2tVTyQviZ68XRd3wACQhAAGgCQEMAAkIYABIAkBDABJCGAASDJjY2jRjk3RuJgkHTlypFYtsnv37lrvw+yKdoEbHBwM33vq1KliLRoRih7K+eSTT4bHbMIDPffs2RPW6z548/jx48VaE8Y4owfMVu36F42aRV832kVtNsYZuQMGgCQEMAAkIYABIAkBDABJCGAASEIAA0ASAhgAkrRlDrhqa7toZre3t7dYm842l9mqZgqj+dPoabHRLG3Vk5jbIdoSs2qbwKgebXMZrVd3d3d4zCbMAVc9gXjbtm21vm4063vo0KFaX7MpoutrbGysWGv3NcIdMAAkIYABIAkBDABJCGAASEIAA0ASAhgAkpi7T/3FZu9KujB77TTCcndfMtUXz5E1kVpYF9ZkcnNkXViTyU26Li0FMABg5vARBAAkIYABIAkBDABJCGAASEIAA0ASAhgAkhDAAJCEAAaAJAQwACRpbACb2WNmdsbMzprZnux+spnZ35jZO2b2WnYvTWFm95jZgJmdNrPXzWxHdk/ZzOzTZvavZnZqYk2eye6pKcysw8xOmtnfZ/fysUYGsJl1SPpLSV+Q1CPpK2bWk9tVuj5Jj2U30TAfSfoTd/+MpIckPcV5ov+W9Dl3XyVptaTHzOyh5J6aYoek09lN3KiRASzpQUln3f2cu38o6RuS8h/Olcjd/0nS1ew+msTdf+bur078/H2NX1xLc7vK5eP+Y+KXn5r4Mec3fDGzZZJ+R9Kz2b3cqKkBvFTSxRt+fUlz/MJCzMy6Ja2R9EpuJ/km/qg9IukdScfdfc6viaQDkv5U0v9kN3KjpgawTfLv5vx/xTE5M7td0jcl7XT3n2f3k83dr7v7aknLJD1oZp/N7imTmf2upHfcvXGPUW9qAF+SdM8Nv14m6c2kXtBgZvYpjYfv1939W9n9NIm7X5M0KP7u4GFJXzKzUY1/nPk5M/vb3JbGNTWA/03Sr5vZCjObL+nLkr6T3BMaxsxM0nOSTrv7X2T30wRmtsTMFk78fIGkjZJ+nNtVLnf/M3df5u7dGs+Sl939q8ltSWpoALv7R5L+SNI/avwvVg67++u5XeUysxck/bOklWZ2ycz+ILunBnhY0tc0fkczMvHji9lNJftVSQNm9u8av5E57u6NGbvCJ/FEDABI0sg7YACYCwhgAEhCAANAEgIYAJIQwACQhAAGgCQEMAAkIYABIAkBDABJCGAASEIAA0ASAhgAkhDAAJCEAAaAJAQwACQhgAEgCQEMAEkIYABIQgADQBICGACSEMAAkIQABoAkBDAAJCGAASAJAQwASQhgAEhCAANAEgIYAJIQwACQhAAGgCQEMAAkIYABIAkBDABJCGAASEIAA0ASAhgAkhDAAJCEAAaAJAQwACQhgAEgCQEMAEnmtfLixYsXe3d3d8sHOXPmTFi/5ZZbirU6x5uO0dFRXblyxab6+rprUiVas+vXrxdrPT09M96LJA0PD19x9yVTeW3dNXn77bfDevR9X7t2rVj74IMPirWOjo7wmPfff3+xNjIyMuU1keqvy8WLF8N69L0vWrSoWLvrrruKtap1KWnX9XP27NmwHp0rK1eubPl401W6floK4O7ubg0NDbV88PXr11d+3ZK+vr6Wjzcdvb29Lb2+7ppUidYsuuBmoxdJMrMLU31t3TU5cOBAWI++76NHjxZrp06dKtZuv/328JgDAwPFWldX15TXRKq/Ljt37gzr0fe+devWWl934cKFlX1Npl3Xz+bNm8N6dK4MDg62fLzpKl0/fAQBAEkIYABIQgADQBICGACSEMAAkKSlKYi6RkdHw/qJEyeKtf7+/mJt+fLltY+Z7dixY2E9WpOnn356ptu5KUR/Mx9NUES16G/Lq47ZLiMjI7XfG00RRdMAGZMC/1d0DVddPxGz8pTcqlWrirXp/D6UcAcMAEkIYABIQgADQBICGACSEMAAkIQABoAkbRlDqxrluXChvKdJZ2dnsVZ3w5qp9DTbpjNKVrURyc2qatOZyN69e4u1aJypCeNWVVavXh3W625mFV0DVetStcHWTKi6hiPr1q0r1qL1avf5wB0wACQhgAEgCQEMAEkIYABIQgADQBICGACSEMAAkKQtc8BVTz2NHpo4NjZWrEXzkdlzvlWqZhyjbfGq5kKbbLa2QKx6oGdJ9EBLKX6oZbtU9bBmzZpiLZqBjq6Rdj+NfKZ7iH5fozn66cwe18EdMAAkIYABIAkBDABJCGAASEIAA0ASAhgAkrRlDK1q1CcaP4qeRLpr1666LU1r68OZUDXuEo3gRCNX0YhN00eLqp46W3dMLTr/2rGt4nRNZzQqerr2+fPni7UmnCvRmFw0pilJXV1dxdqOHTuKtegcrHrSep014w4YAJIQwACQhAAGgCQEMAAkIYABIAkBDABJ2jKGVmU2RoGqRkayVY2sROND0VhSNJp38uTJ8Jjt2GUt+r6rxhXNrNZ7b4ZRs2j8acOGDeF7oydsR9dBNLJY9XuRPaZWNbIY1eue51Wjq1VrNhnugAEgCQEMAEkIYABIQgADQBICGACSEMAAkKQtY2jHjh0L652dncXa3r17ax0zGrFpgqoHLUbjZNEIUDR2VDUmk/2wz6oxn+g8Wbdu3Uy301bR72n0fUvxukXnQ/Qwz76+vvCYda/LdonO5Wi9ou+7zphZFe6AASAJAQwASQhgAEhCAANAEgIYAJIQwACQhAAGgCRtmQMeGBgI6wcPHqz1dbds2VKsNX0Lwqo54Gh+M5pVjL7vps9GVz31uL+/v1iLnqB7M4j6rzqXoycARzPEmzZtKtaynxpepaq/aDvKaDvX6BycjTl57oABIAkBDABJCGAASEIAA0ASAhgAkhDAAJDE3H3qLzZ7V9KF2WunEZa7+5KpvniOrInUwrqwJpObI+vCmkxu0nVpKYABADOHjyAAIAkBDABJCGAASEIAA0ASAhgAkhDAAJCEAAaAJAQwACQhgAEgSWMD2MxGzeyHZjZiZkPZ/TSBmS00sxfN7MdmdtrMfjO7p0xmtnLi/Pj4x8/NrNmPcmgDM9tlZq+b2Wtm9oKZfTq7pyYwsx0Ta/J6U86Txv6vyGY2KqnX3a9k99IUZtYv6fvu/qyZzZd0q7uXn68yh5hZh6TLkn7D3efC3gKTMrOlkn4gqcfdPzCzw5L+wd37cjvLZWaflfQNSQ9K+lDSdyX9obv/JLOvxt4B45PM7A5Jj0h6TpLc/UPC9xM+L+mNuRy+N5gnaYGZzZN0q6Q3k/tpgs9I+hd3/y93/0jSCUm/l9xTowPYJX3PzIbNbFt2Mw1wr6R3JT1vZifN7Fkzuy27qQb5sqQXspvI5u6XJf25pJ9K+pmkMXf/Xm5XjfCapEfMbJGZ3Srpi5LuSe6p0QH8sLuvlfQFSU+Z2SPZDSWbJ2mtpL9y9zWS/lPSntyWmmHi45gvSTqS3Us2M+uStEnSCkl3S7rNzL6a21U+dz8tab+k4xr/+OGUpI9Sm1KDA9jd35z45zuSvq3xz27mskuSLrn7KxO/flHjgYzx/0i/6u5vZzfSABslnXf3d939F5K+Jem3kntqBHd/zt3Xuvsjkq5KSv38V2poAJvZbWb2yx//XNJva/yPEHOWu78l6aKZrZz4V5+X9KPElprkK+Ljh4/9VNJDZnarmZnGz5PTyT01gpn9ysQ/f03S76sB58y87AYK7pL07fHzR/Mk/Z27fze3pUb4Y0lfn/gj9zlJTyb3k27i87xHJW3P7qUJ3P0VM3tR0qsa/yP2SUl/ndtVY3zTzBZJ+oWkp9z9veyGGjuGBgD/3zXyIwgAmAsIYABIQgADQBICGACSEMAAkIQABoAkBDAAJPlfVgtnJDs9l8EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(1,11):\n",
    "    plt.subplot(2,5,i)\n",
    "    #数据集前9个数据正好是1~9\n",
    "    plt.imshow(digits.data[i-1].reshape([8,8]),cmap=plt.cm.gray_r)\n",
    "    #（3，10）为文本位置的坐标。每个框的左上角是（0，0），下边框是8因为像素是8x8\n",
    "    plt.text(3,10,str(digits.target[i-1]))\n",
    "    #关闭坐标轴刻度\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1347, 64)\n",
      "(450, 64)\n",
      "(1347,)\n",
      "(450,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.25)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 接下来是几种运算的正向传播和反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear/affine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正向"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$ z = w^T * x +b $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_forward(x, w, b):\n",
    "  \"\"\"\n",
    "  输入：\n",
    "  x的维度为(N, d_1, ..., d_k)。N是batchsize，每个样本有k个特征。我们可以将其reshape为（N，D）\n",
    "  w的维度为(D, M)，M为下一层神经元的个数(对应输出维度)\n",
    "  b的维度为(M,)\n",
    "  输出：\n",
    "  out的维度为(N, M)，作为下一层的输入\n",
    "  cache元组(x, w, b)存下反向传播中可以直接导入计算梯度\n",
    "  \"\"\"\n",
    "  out = None\n",
    "  N, D = x.shape[0], int(x.size / x.shape[0])\n",
    "  out = np.dot(x.reshape(N, D), w) + b\n",
    "  cache = (x, w, b)\n",
    "  return out, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 反向"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial{dz}}{\\partial{dx}} =dz*w^T$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial{dz}}{\\partial{dw}} =x^T*dz$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial{dz}}{\\partial{db}} =\\sum_{i=1}^{N}{dz(i,:)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后这个偏导写的pythonic了，一时想不到这个reshape数学上应该怎么写= =懂意思就行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_backward(dz, cache):\n",
    "  \"\"\"\n",
    "  输入:\n",
    "  dz是上一层返回的梯度，这里维度视为(N, M)\n",
    "  cache元组(x, w, b)为正向传播中存下的参数\n",
    "  输出:\n",
    "  输出为dz关于x，w，b的偏导dx，dw，db\n",
    "  dx的维度为(N, d_1, ..., d_k)\n",
    "  dw的维度为(D, M)\n",
    "  db的维度为(M,)\n",
    "  \"\"\"\n",
    "  x, w, b = cache\n",
    "  dx, dw, db = None, None, None\n",
    "  pass\n",
    "  N, D = x.shape[0], w.shape[0]\n",
    "  dx = np.dot(dz, w.T).reshape(x.shape)\n",
    "  dw = np.dot(x.reshape(N, D).T, dz)\n",
    "  db = np.sum(dz, axis = 0)\n",
    "\n",
    "  return dx, dw, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里就直接RELU吧，不用sigmoid了。下一个就是softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正向"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$ out = RELU(z)=max(0,z)$$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_forward(x):\n",
    "  \"\"\"\n",
    "  relu就是取正数部分不用多解释啦\n",
    "  \"\"\"\n",
    "  out = None\n",
    "  out = np.maximum(0, x)\n",
    "  cache = x\n",
    "  return out, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 反向"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial{dout}}{\\partial{dx}} =dout*bool(x>0)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  输入:\n",
    "  dout:上一步反向传播回来的梯度\n",
    "  cache:正向传播时记录下的参数\n",
    "  输出:\n",
    "  dout对于d的偏导\n",
    "  \"\"\"\n",
    "  dx, x = None, cache\n",
    "\n",
    "  dx = dout * (x > 0)\n",
    "\n",
    "  return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正向+反向"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为这一步是传播的终点，所以我们可以把正向和反向写在一个函数里"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$S(a_i) = \\frac{e^{a_i}}{\\sum_{j=1}^C e^{{a_j}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$l=-\\sum_{i=1}^{C}y^{(i)}log(S(a_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L=\\frac{1}{N}\\sum_{i=1}^{N}l$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了便于公式书写(否则又要写pythonic了)以上的S(a)是(1,C)的向量，y是0或1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dx反向传播求解步骤有点长这里我就不贴了，网上softmax反向传播能搜到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(x, y):\n",
    "  \"\"\"\n",
    "  输入:\n",
    "  x的维度为(N, C)，对应N个样本C个分类。x[i, j]代表第i个样本在第j个分类的得分\n",
    "  y的维度为(N,)，代表样本的label，为0~C-1的整数\n",
    "  输出:\n",
    "  loss是一个标量对应上面的L\n",
    "  dx是loss对于x的偏导，维度(N, C)。\n",
    "   \n",
    "  \"\"\"\n",
    "  #x - np.max是为了防止数值上溢\n",
    "  S = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "  #逐行对当前行求和(单个样本softmax)\n",
    "  S /= np.sum(S, axis=1, keepdims=True)\n",
    "  N = x.shape[0]\n",
    "  #逐列对当前列求和(所有样本的损失累加)，对于每个样本每一行只有对应y列(label)的log值才会被计入损失\n",
    "  loss = -np.sum(np.log(S[np.arange(N), y])) / N\n",
    "  dx = S.copy()\n",
    "  dx[np.arange(N), y] -= 1\n",
    "  dx /= N\n",
    "  return loss, dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优化器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里先只做一个SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\theta = \\theta - \\alpha*d\\theta$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class optim(object):\n",
    "    def sgd(self,w, dw, config=None):\n",
    "          \"\"\"\n",
    "          输入:\n",
    "          w就是对应公式中的θ要优化的参数，dw是当前层返回梯度，config是一个包含所有超参数的字典\n",
    "          输出:\n",
    "          更新过后的w\n",
    "          \"\"\"\n",
    "          if config is None: config = {}\n",
    "          config.setdefault('learning_rate', 1e-2)\n",
    "          w -= config['learning_rate'] * dw\n",
    "          return w, config\n",
    "    \n",
    "optim=optim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 封装模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到这里时可以思考下之后要封装的模型了。不如我们模仿Keras，按照其提供的接口来封装模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仿照Keras，model实例化的时候传入模型的结构，model.compile传入损失函数、优化器、学习率，model.fit传入数据集完成训练过程同时记录损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras三板斧，这里我就不运行了\n",
    "#见https://keras.io/getting-started/functional-api-guide/\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(data, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class FullyConnected(object):\n",
    "    def __init__(self, hidden_dims, input_dim=8*8, num_classes=10,\n",
    "               reg=0.0, weight_scale=1e-2, dtype=np.float32):\n",
    "        \"\"\"\n",
    "        hidden_dims为列表表示模型的结构，如[3,2,1]即代表第一层隐藏层3个神经元，第二层隐藏层2个神经元，第三层隐藏层1个神经元\n",
    "        input_dim输入维度，默认对应mnist输入\n",
    "        num_classes等同于输出层神经元个数，默认对应mnist类别\n",
    "        reg为正则化系数\n",
    "        weight_scale为系数的初始化\n",
    "        dtype=np.float32代表所有系数精度为float32(也可以设为float64)\n",
    "        \"\"\"\n",
    "        self.reg = reg\n",
    "        #总层数算上输出层\n",
    "        self.num_layers = 1 + len(hidden_dims)\n",
    "        self.dtype = dtype\n",
    "        #params字典用于存储系数\n",
    "        self.params = {}\n",
    "        #初始化第一层系数，这里W按照正态分布\n",
    "        self.params['W1'] = np.random.randn(input_dim, hidden_dims[0]) * weight_scale\n",
    "        self.params['b1'] = np.zeros(hidden_dims[0])\n",
    "        #初始化第二层开始的系数\n",
    "        for i in range(self.num_layers - 2):\n",
    "            self.params['W' + str(i+2)] = np.random.randn(hidden_dims[i], hidden_dims[i+1]) * weight_scale\n",
    "            self.params['b' + str(i+2)] = np.zeros(hidden_dims[i+1])\n",
    "        #初始化最后一层的系数\n",
    "        self.params['W' + str(self.num_layers)] = np.random.randn(hidden_dims[-1], num_classes)\n",
    "        self.params['b' + str(self.num_layers)] = np.random.randn(num_classes)\n",
    "        #确定系数精度\n",
    "        for k, v in self.params.items():\n",
    "            self.params[k] = v.astype(dtype)\n",
    "\n",
    "    def loss(self, x, y=None):\n",
    "        \"\"\"\n",
    "        输入：\n",
    "        x的维度为(N, d_1, ..., d_k)。N是batchsize，每个样本有k个特征。我们可以将其reshape为（N，D）\n",
    "        y的维度为(N,)，代表样本的label，y也可以为None\n",
    "        输出:\n",
    "        当y不为None时，说明这是训练的过程，返回loss，梯度\n",
    "        当y为None的时候，说明这是测试的过程，返回score\n",
    "        调用fit会用到这个方法\n",
    "        \"\"\"\n",
    "        x = x.astype(self.dtype)\n",
    "        mode = 'test' if y is None else 'train'\n",
    "        scores = None\n",
    "        #初始化两个列表，用来存每一层的输出和系数\n",
    "        hidden_layers, caches = list(range(self.num_layers + 1)), list(range(self.num_layers))\n",
    "        hidden_layers[0] = x\n",
    "        for i in range(self.num_layers):\n",
    "            w, b = self.params['W' + str(i+1)], self.params['b' + str(i+1)]\n",
    "            #print(i,w.shape)\n",
    "            #最后一层没有RELU\n",
    "            if i == self.num_layers - 1:\n",
    "                hidden_layers[i+1], caches[i] = affine_forward(hidden_layers[i], w, b)\n",
    "            else:\n",
    "                a, fc_cache = affine_forward(hidden_layers[i], w, b)\n",
    "                out, relu_cache = relu_forward(a)\n",
    "                #print(a.shape,out.shape)\n",
    "                cache = (fc_cache, relu_cache)\n",
    "                hidden_layers[i+1], caches[i] = out, cache\n",
    "        #这里的score是没经过softmax的\n",
    "        scores = hidden_layers[self.num_layers]   \n",
    "    \n",
    "        if mode == 'test':\n",
    "            return scores\n",
    "\n",
    "        loss, grads = 0.0, {}\n",
    "        loss, dscores = softmax_loss(scores, y)\n",
    "        #初始化列表存储每一层梯度\n",
    "        dhiddens = list(range(self.num_layers + 1))\n",
    "        dhiddens[self.num_layers] = dscores\n",
    "        for i in range(self.num_layers, 0, -1):\n",
    "            #最后一层没有RELU\n",
    "            if i == self.num_layers:\n",
    "                dhiddens[i-1], grads['W' + str(i)], grads['b'+str(i)] = affine_backward(dhiddens[i], caches[i-1])\n",
    "            else:\n",
    "                  fc_cache, relu_cache = caches[i-1]\n",
    "                  da = relu_backward(dhiddens[i], relu_cache)\n",
    "                  dx, dw, db = affine_backward(da, fc_cache)\n",
    "                  dhiddens[i-1], grads['W' + str(i)], grads['b' + str(i)] = dx, dw, db\n",
    "            #L2正则化\n",
    "            loss += 0.5 * self.reg * np.sum(self.params['W' + str(i)] ** 2)\n",
    "            grads['W' + str(i)] += self.reg * self.params['W' + str(i)]\n",
    "\n",
    "        return loss, grads\n",
    "\n",
    "    def compile(self, **kwargs):\n",
    "        \"\"\"\n",
    "        记录优化器相关的超参数，同时调用_reset初始化训练记录\n",
    "        \"\"\"        \n",
    "        self.update_rule = kwargs.pop('update_rule', 'sgd')\n",
    "        self.optim_config = kwargs.pop('optim_config', {})\n",
    "        self.lr_decay = kwargs.pop('lr_decay', 1.0)\n",
    " \n",
    "        if len(kwargs) > 0:\n",
    "              extra = ', '.join('\"%s\"' % k for k in kwargs.keys())\n",
    "              raise ValueError('传入参数 %s未识别' % extra)\n",
    "\n",
    "        if not hasattr(optim, self.update_rule):\n",
    "              raise ValueError('缺少有效的优化器 \"%s\"' % self.update_rule)\n",
    "        self.update_rule = getattr(optim, self.update_rule)\n",
    "\n",
    "        self._reset()\n",
    "        \n",
    "    def _reset(self):\n",
    "        \"\"\"\n",
    "        初始化训练过程的记录，compile调用\n",
    "        \"\"\"\n",
    "        self.epoch = 0\n",
    "        self.best_val_acc = 0\n",
    "        self.best_params = {}\n",
    "        self.loss_history = []\n",
    "        self.train_acc_history = []\n",
    "        self.val_acc_history = []\n",
    "        #记录每一个参数的optim_config(optim_config为学习率，优化器相关的参数)\n",
    "        self.optim_configs = {}\n",
    "        for p in self.params:\n",
    "              d = {k: v for k, v in self.optim_config.items()}\n",
    "              self.optim_configs[p] = d\n",
    "            \n",
    "    def fit(self,x_train,y_train,x_val,y_val,epochs=30,batch_size=64,verbose=1):\n",
    "        \"\"\"\n",
    "        训练过程，按照verbose的输入决定是否在epoch结束打印信息。最后保存模型(训练得到的最佳参数)\n",
    "        \"\"\"\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_val = x_val\n",
    "        self.y_val = y_val   \n",
    "        self.num_epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose=verbose\n",
    "        num_train = self.x_train.shape[0]\n",
    "        #print(num_train,self.batch_size)\n",
    "        iterations_per_epoch = int(max(num_train / self.batch_size, 1))\n",
    "        #print('iterations_per_epoch %f'%iterations_per_epoch)\n",
    "        #总迭代次数\n",
    "        num_iterations = self.num_epochs * iterations_per_epoch\n",
    "        #print (int(num_iterations))\n",
    "        for t in range(int(num_iterations)):\n",
    "            #print(t)\n",
    "            self._step()\n",
    "            epoch_end = (t + 1) % iterations_per_epoch == 0\n",
    "            if epoch_end:\n",
    "                self.epoch += 1\n",
    "                #学习率衰减\n",
    "                for k in self.optim_configs:\n",
    "                      self.optim_configs[k]['learning_rate'] *= self.lr_decay\n",
    "            # 第一次迭代，最后一次迭代，或者一个epoch记录准确率\n",
    "            first_it = (t == 0)\n",
    "            last_it = (t == num_iterations + 1)\n",
    "            if first_it or last_it or epoch_end:\n",
    "                train_acc = self.check_accuracy(self.x_train, self.y_train,num_samples=1000)\n",
    "                val_acc = self.check_accuracy(self.x_val, self.y_val)\n",
    "                self.train_acc_history.append(train_acc)\n",
    "                self.val_acc_history.append(val_acc)\n",
    "\n",
    "                if self.verbose:\n",
    "                      print ('epoch %d / %d loss: %f train acc: %f; val_acc: %f' % (self.epoch, self.num_epochs,self.loss_history[-1],\n",
    "                                                                                    train_acc, val_acc))\n",
    "\n",
    "                # 记录最佳模型\n",
    "                if val_acc > self.best_val_acc:\n",
    "                      self.best_val_acc = val_acc\n",
    "                      self.best_params = {}\n",
    "                      #记录参数\n",
    "                      for k, v in self.params.items():\n",
    "                            self.best_params[k] = v.copy()\n",
    "        self.params = self.best_params\n",
    "\n",
    "    def _step(self):\n",
    "        \"\"\"\n",
    "        单次优化，记录下loss_history，fit调用\n",
    "        \"\"\"\n",
    "        # 采样batch\n",
    "        num_train = self.x_train.shape[0]\n",
    "        batch_mask = np.random.choice(num_train, self.batch_size)\n",
    "        X_batch = self.x_train[batch_mask]\n",
    "        y_batch = self.y_train[batch_mask]\n",
    "\n",
    "        # 前向传播反向传播记录loss\n",
    "        loss, grads = self.loss(X_batch, y_batch)\n",
    "        self.loss_history.append(loss)\n",
    "\n",
    "        # 更新参数\n",
    "        for p, w in self.params.items():\n",
    "              dw = grads[p]\n",
    "              #学习率等参数传入config\n",
    "              config = self.optim_configs[p]\n",
    "              next_w, next_config = self.update_rule(w, dw, config)\n",
    "              self.params[p] = next_w\n",
    "              self.optim_configs[p] = next_config\n",
    "            \n",
    "    def check_accuracy(self, x, y, num_samples=None, batch_size=64):\n",
    "        \"\"\"\n",
    "        根据输入的数据计算模型的准确率，num_samples不为空的话即测试num_samples个样本。fit调用\n",
    "        \"\"\"\n",
    "        N = x.shape[0]\n",
    "        if num_samples is not None and N > num_samples:\n",
    "              mask = np.random.choice(N, num_samples)\n",
    "              N = num_samples\n",
    "              x = x[mask]\n",
    "              y = y[mask]\n",
    "        num_batches = N / batch_size\n",
    "        if N % batch_size != 0:\n",
    "              num_batches += 1\n",
    "        y_pred = []\n",
    "        for i in range(int(num_batches)):\n",
    "              start = i * batch_size\n",
    "              end = (i + 1) * batch_size\n",
    "              scores = self.loss(x[start:end])\n",
    "              #模型预测结果\n",
    "              y_pred.append(np.argmax(scores, axis=1))\n",
    "        y_pred = np.hstack(y_pred)\n",
    "        acc = np.mean(y_pred == y)\n",
    "\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "试试我们的模型吧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FullyConnected([100, 100, 100, 100],weight_scale=3e-2, dtype=np.float64)\n",
    "model.compile(\n",
    "              update_rule='sgd',\n",
    "              optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "              }\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再看一眼我们的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1347, 64)\n",
      "(450, 64)\n",
      "(1347,)\n",
      "(450,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 / 100 loss: 3.072079 train acc: 0.057000; val_acc: 0.060000\n",
      "epoch 1 / 100 loss: 2.660091 train acc: 0.117000; val_acc: 0.106667\n",
      "epoch 2 / 100 loss: 2.812931 train acc: 0.214000; val_acc: 0.204444\n",
      "epoch 3 / 100 loss: 2.285188 train acc: 0.187000; val_acc: 0.180000\n",
      "epoch 4 / 100 loss: 2.311214 train acc: 0.204000; val_acc: 0.186667\n",
      "epoch 5 / 100 loss: 2.401614 train acc: 0.262000; val_acc: 0.244444\n",
      "epoch 6 / 100 loss: 2.040300 train acc: 0.376000; val_acc: 0.375556\n",
      "epoch 7 / 100 loss: 1.912909 train acc: 0.467000; val_acc: 0.453333\n",
      "epoch 8 / 100 loss: 1.868188 train acc: 0.625000; val_acc: 0.595556\n",
      "epoch 9 / 100 loss: 1.615804 train acc: 0.664000; val_acc: 0.648889\n",
      "epoch 10 / 100 loss: 1.457032 train acc: 0.732000; val_acc: 0.728889\n",
      "epoch 11 / 100 loss: 1.270667 train acc: 0.736000; val_acc: 0.735556\n",
      "epoch 12 / 100 loss: 1.122568 train acc: 0.750000; val_acc: 0.771111\n",
      "epoch 13 / 100 loss: 1.007589 train acc: 0.785000; val_acc: 0.784444\n",
      "epoch 14 / 100 loss: 0.911566 train acc: 0.835000; val_acc: 0.840000\n",
      "epoch 15 / 100 loss: 0.790561 train acc: 0.855000; val_acc: 0.855556\n",
      "epoch 16 / 100 loss: 0.634261 train acc: 0.880000; val_acc: 0.862222\n",
      "epoch 17 / 100 loss: 0.869241 train acc: 0.888000; val_acc: 0.895556\n",
      "epoch 18 / 100 loss: 0.494058 train acc: 0.901000; val_acc: 0.920000\n",
      "epoch 19 / 100 loss: 0.604128 train acc: 0.903000; val_acc: 0.926667\n",
      "epoch 20 / 100 loss: 0.521962 train acc: 0.903000; val_acc: 0.920000\n",
      "epoch 21 / 100 loss: 0.288940 train acc: 0.910000; val_acc: 0.935556\n",
      "epoch 22 / 100 loss: 0.349689 train acc: 0.917000; val_acc: 0.935556\n",
      "epoch 23 / 100 loss: 0.332625 train acc: 0.919000; val_acc: 0.937778\n",
      "epoch 24 / 100 loss: 0.395903 train acc: 0.927000; val_acc: 0.940000\n",
      "epoch 25 / 100 loss: 0.189570 train acc: 0.937000; val_acc: 0.944444\n",
      "epoch 26 / 100 loss: 0.516194 train acc: 0.932000; val_acc: 0.942222\n",
      "epoch 27 / 100 loss: 0.268353 train acc: 0.936000; val_acc: 0.944444\n",
      "epoch 28 / 100 loss: 0.346236 train acc: 0.936000; val_acc: 0.951111\n",
      "epoch 29 / 100 loss: 0.496544 train acc: 0.913000; val_acc: 0.951111\n",
      "epoch 30 / 100 loss: 0.164683 train acc: 0.938000; val_acc: 0.951111\n",
      "epoch 31 / 100 loss: 0.181099 train acc: 0.929000; val_acc: 0.946667\n",
      "epoch 32 / 100 loss: 0.185870 train acc: 0.959000; val_acc: 0.953333\n",
      "epoch 33 / 100 loss: 0.204607 train acc: 0.936000; val_acc: 0.953333\n",
      "epoch 34 / 100 loss: 0.159376 train acc: 0.944000; val_acc: 0.946667\n",
      "epoch 35 / 100 loss: 0.197147 train acc: 0.949000; val_acc: 0.942222\n",
      "epoch 36 / 100 loss: 0.110824 train acc: 0.957000; val_acc: 0.946667\n",
      "epoch 37 / 100 loss: 0.156777 train acc: 0.954000; val_acc: 0.955556\n",
      "epoch 38 / 100 loss: 0.176129 train acc: 0.957000; val_acc: 0.953333\n",
      "epoch 39 / 100 loss: 0.366314 train acc: 0.959000; val_acc: 0.962222\n",
      "epoch 40 / 100 loss: 0.149780 train acc: 0.965000; val_acc: 0.957778\n",
      "epoch 41 / 100 loss: 0.195076 train acc: 0.960000; val_acc: 0.962222\n",
      "epoch 42 / 100 loss: 0.142693 train acc: 0.960000; val_acc: 0.960000\n",
      "epoch 43 / 100 loss: 0.112310 train acc: 0.965000; val_acc: 0.957778\n",
      "epoch 44 / 100 loss: 0.131151 train acc: 0.968000; val_acc: 0.962222\n",
      "epoch 45 / 100 loss: 0.081184 train acc: 0.963000; val_acc: 0.948889\n",
      "epoch 46 / 100 loss: 0.165903 train acc: 0.966000; val_acc: 0.966667\n",
      "epoch 47 / 100 loss: 0.082711 train acc: 0.955000; val_acc: 0.960000\n",
      "epoch 48 / 100 loss: 0.212922 train acc: 0.972000; val_acc: 0.955556\n",
      "epoch 49 / 100 loss: 0.134187 train acc: 0.971000; val_acc: 0.960000\n",
      "epoch 50 / 100 loss: 0.189331 train acc: 0.964000; val_acc: 0.957778\n",
      "epoch 51 / 100 loss: 0.127188 train acc: 0.961000; val_acc: 0.960000\n",
      "epoch 52 / 100 loss: 0.147577 train acc: 0.975000; val_acc: 0.957778\n",
      "epoch 53 / 100 loss: 0.110537 train acc: 0.979000; val_acc: 0.960000\n",
      "epoch 54 / 100 loss: 0.161519 train acc: 0.972000; val_acc: 0.964444\n",
      "epoch 55 / 100 loss: 0.060905 train acc: 0.972000; val_acc: 0.966667\n",
      "epoch 56 / 100 loss: 0.048206 train acc: 0.972000; val_acc: 0.964444\n",
      "epoch 57 / 100 loss: 0.099738 train acc: 0.974000; val_acc: 0.953333\n",
      "epoch 58 / 100 loss: 0.054517 train acc: 0.971000; val_acc: 0.964444\n",
      "epoch 59 / 100 loss: 0.141267 train acc: 0.971000; val_acc: 0.964444\n",
      "epoch 60 / 100 loss: 0.183813 train acc: 0.980000; val_acc: 0.960000\n",
      "epoch 61 / 100 loss: 0.087541 train acc: 0.977000; val_acc: 0.973333\n",
      "epoch 62 / 100 loss: 0.044238 train acc: 0.975000; val_acc: 0.962222\n",
      "epoch 63 / 100 loss: 0.091196 train acc: 0.985000; val_acc: 0.951111\n",
      "epoch 64 / 100 loss: 0.066016 train acc: 0.980000; val_acc: 0.957778\n",
      "epoch 65 / 100 loss: 0.124180 train acc: 0.982000; val_acc: 0.957778\n",
      "epoch 66 / 100 loss: 0.058294 train acc: 0.979000; val_acc: 0.966667\n",
      "epoch 67 / 100 loss: 0.178784 train acc: 0.976000; val_acc: 0.966667\n",
      "epoch 68 / 100 loss: 0.100486 train acc: 0.976000; val_acc: 0.957778\n",
      "epoch 69 / 100 loss: 0.125238 train acc: 0.990000; val_acc: 0.968889\n",
      "epoch 70 / 100 loss: 0.040501 train acc: 0.979000; val_acc: 0.955556\n",
      "epoch 71 / 100 loss: 0.043784 train acc: 0.986000; val_acc: 0.966667\n",
      "epoch 72 / 100 loss: 0.064622 train acc: 0.985000; val_acc: 0.973333\n",
      "epoch 73 / 100 loss: 0.079644 train acc: 0.983000; val_acc: 0.962222\n",
      "epoch 74 / 100 loss: 0.086760 train acc: 0.978000; val_acc: 0.964444\n",
      "epoch 75 / 100 loss: 0.087019 train acc: 0.984000; val_acc: 0.966667\n",
      "epoch 76 / 100 loss: 0.076932 train acc: 0.985000; val_acc: 0.960000\n",
      "epoch 77 / 100 loss: 0.107343 train acc: 0.986000; val_acc: 0.951111\n",
      "epoch 78 / 100 loss: 0.029397 train acc: 0.986000; val_acc: 0.964444\n",
      "epoch 79 / 100 loss: 0.031125 train acc: 0.979000; val_acc: 0.955556\n",
      "epoch 80 / 100 loss: 0.056470 train acc: 0.988000; val_acc: 0.962222\n",
      "epoch 81 / 100 loss: 0.093848 train acc: 0.991000; val_acc: 0.960000\n",
      "epoch 82 / 100 loss: 0.076352 train acc: 0.991000; val_acc: 0.964444\n",
      "epoch 83 / 100 loss: 0.050415 train acc: 0.990000; val_acc: 0.966667\n",
      "epoch 84 / 100 loss: 0.068520 train acc: 0.992000; val_acc: 0.964444\n",
      "epoch 85 / 100 loss: 0.069125 train acc: 0.995000; val_acc: 0.968889\n",
      "epoch 86 / 100 loss: 0.039661 train acc: 0.981000; val_acc: 0.955556\n",
      "epoch 87 / 100 loss: 0.157629 train acc: 0.991000; val_acc: 0.962222\n",
      "epoch 88 / 100 loss: 0.025658 train acc: 0.992000; val_acc: 0.966667\n",
      "epoch 89 / 100 loss: 0.041298 train acc: 0.995000; val_acc: 0.960000\n",
      "epoch 90 / 100 loss: 0.021180 train acc: 0.988000; val_acc: 0.964444\n",
      "epoch 91 / 100 loss: 0.082906 train acc: 0.990000; val_acc: 0.962222\n",
      "epoch 92 / 100 loss: 0.022931 train acc: 0.980000; val_acc: 0.962222\n",
      "epoch 93 / 100 loss: 0.078400 train acc: 0.988000; val_acc: 0.960000\n",
      "epoch 94 / 100 loss: 0.028201 train acc: 0.984000; val_acc: 0.960000\n",
      "epoch 95 / 100 loss: 0.060024 train acc: 0.987000; val_acc: 0.966667\n",
      "epoch 96 / 100 loss: 0.030021 train acc: 0.986000; val_acc: 0.951111\n",
      "epoch 97 / 100 loss: 0.046263 train acc: 0.990000; val_acc: 0.964444\n",
      "epoch 98 / 100 loss: 0.151802 train acc: 0.994000; val_acc: 0.962222\n",
      "epoch 99 / 100 loss: 0.032607 train acc: 0.986000; val_acc: 0.955556\n",
      "epoch 100 / 100 loss: 0.029640 train acc: 0.991000; val_acc: 0.955556\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train,X_test,y_test,epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model.val_acc_history, 'o')\n",
    "plt.title('val_acc_history history')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('val_acc_history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改变学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 / 100 loss: 2.857777 train acc: 0.087000; val_acc: 0.097778\n",
      "epoch 1 / 100 loss: 2.868165 train acc: 0.104000; val_acc: 0.097778\n",
      "epoch 2 / 100 loss: 2.830043 train acc: 0.095000; val_acc: 0.097778\n",
      "epoch 3 / 100 loss: 2.774539 train acc: 0.129000; val_acc: 0.097778\n",
      "epoch 4 / 100 loss: 2.835958 train acc: 0.112000; val_acc: 0.097778\n",
      "epoch 5 / 100 loss: 2.668227 train acc: 0.109000; val_acc: 0.097778\n",
      "epoch 6 / 100 loss: 2.836369 train acc: 0.103000; val_acc: 0.097778\n",
      "epoch 7 / 100 loss: 2.800582 train acc: 0.113000; val_acc: 0.097778\n",
      "epoch 8 / 100 loss: 2.385211 train acc: 0.103000; val_acc: 0.097778\n",
      "epoch 9 / 100 loss: 2.706833 train acc: 0.100000; val_acc: 0.097778\n",
      "epoch 10 / 100 loss: 2.607915 train acc: 0.106000; val_acc: 0.097778\n",
      "epoch 11 / 100 loss: 2.271246 train acc: 0.092000; val_acc: 0.097778\n",
      "epoch 12 / 100 loss: 2.545260 train acc: 0.104000; val_acc: 0.097778\n",
      "epoch 13 / 100 loss: 2.527246 train acc: 0.108000; val_acc: 0.100000\n",
      "epoch 14 / 100 loss: 2.543498 train acc: 0.096000; val_acc: 0.095556\n",
      "epoch 15 / 100 loss: 2.358079 train acc: 0.105000; val_acc: 0.100000\n",
      "epoch 16 / 100 loss: 2.468740 train acc: 0.150000; val_acc: 0.115556\n",
      "epoch 17 / 100 loss: 2.430213 train acc: 0.144000; val_acc: 0.137778\n",
      "epoch 18 / 100 loss: 2.619439 train acc: 0.149000; val_acc: 0.155556\n",
      "epoch 19 / 100 loss: 2.413722 train acc: 0.203000; val_acc: 0.186667\n",
      "epoch 20 / 100 loss: 2.376833 train acc: 0.220000; val_acc: 0.215556\n",
      "epoch 21 / 100 loss: 2.401484 train acc: 0.255000; val_acc: 0.222222\n",
      "epoch 22 / 100 loss: 2.222710 train acc: 0.234000; val_acc: 0.224444\n",
      "epoch 23 / 100 loss: 2.742141 train acc: 0.216000; val_acc: 0.228889\n",
      "epoch 24 / 100 loss: 2.311604 train acc: 0.239000; val_acc: 0.242222\n",
      "epoch 25 / 100 loss: 2.428213 train acc: 0.245000; val_acc: 0.251111\n",
      "epoch 26 / 100 loss: 2.405897 train acc: 0.259000; val_acc: 0.260000\n",
      "epoch 27 / 100 loss: 2.357076 train acc: 0.266000; val_acc: 0.255556\n",
      "epoch 28 / 100 loss: 2.302794 train acc: 0.258000; val_acc: 0.253333\n",
      "epoch 29 / 100 loss: 2.148688 train acc: 0.239000; val_acc: 0.251111\n",
      "epoch 30 / 100 loss: 2.359654 train acc: 0.254000; val_acc: 0.262222\n",
      "epoch 31 / 100 loss: 2.320744 train acc: 0.274000; val_acc: 0.275556\n",
      "epoch 32 / 100 loss: 2.375028 train acc: 0.252000; val_acc: 0.273333\n",
      "epoch 33 / 100 loss: 2.222984 train acc: 0.264000; val_acc: 0.268889\n",
      "epoch 34 / 100 loss: 2.124851 train acc: 0.270000; val_acc: 0.266667\n",
      "epoch 35 / 100 loss: 2.148173 train acc: 0.264000; val_acc: 0.266667\n",
      "epoch 36 / 100 loss: 2.239816 train acc: 0.271000; val_acc: 0.266667\n",
      "epoch 37 / 100 loss: 2.105617 train acc: 0.307000; val_acc: 0.288889\n",
      "epoch 38 / 100 loss: 2.173018 train acc: 0.295000; val_acc: 0.300000\n",
      "epoch 39 / 100 loss: 2.116031 train acc: 0.289000; val_acc: 0.302222\n",
      "epoch 40 / 100 loss: 2.113574 train acc: 0.324000; val_acc: 0.315556\n",
      "epoch 41 / 100 loss: 2.086621 train acc: 0.320000; val_acc: 0.331111\n",
      "epoch 42 / 100 loss: 2.051345 train acc: 0.304000; val_acc: 0.335556\n",
      "epoch 43 / 100 loss: 2.195632 train acc: 0.350000; val_acc: 0.346667\n",
      "epoch 44 / 100 loss: 1.967487 train acc: 0.344000; val_acc: 0.346667\n",
      "epoch 45 / 100 loss: 2.033320 train acc: 0.342000; val_acc: 0.360000\n",
      "epoch 46 / 100 loss: 2.119565 train acc: 0.348000; val_acc: 0.377778\n",
      "epoch 47 / 100 loss: 2.019817 train acc: 0.361000; val_acc: 0.368889\n",
      "epoch 48 / 100 loss: 2.119277 train acc: 0.400000; val_acc: 0.404444\n",
      "epoch 49 / 100 loss: 2.039895 train acc: 0.433000; val_acc: 0.422222\n",
      "epoch 50 / 100 loss: 1.913423 train acc: 0.427000; val_acc: 0.437778\n",
      "epoch 51 / 100 loss: 1.986525 train acc: 0.436000; val_acc: 0.462222\n",
      "epoch 52 / 100 loss: 1.978886 train acc: 0.460000; val_acc: 0.460000\n",
      "epoch 53 / 100 loss: 2.030416 train acc: 0.465000; val_acc: 0.455556\n",
      "epoch 54 / 100 loss: 2.019798 train acc: 0.469000; val_acc: 0.473333\n",
      "epoch 55 / 100 loss: 2.041802 train acc: 0.497000; val_acc: 0.488889\n",
      "epoch 56 / 100 loss: 1.901087 train acc: 0.520000; val_acc: 0.482222\n",
      "epoch 57 / 100 loss: 1.873161 train acc: 0.552000; val_acc: 0.493333\n",
      "epoch 58 / 100 loss: 1.850662 train acc: 0.523000; val_acc: 0.500000\n",
      "epoch 59 / 100 loss: 1.936594 train acc: 0.558000; val_acc: 0.506667\n",
      "epoch 60 / 100 loss: 1.886596 train acc: 0.549000; val_acc: 0.513333\n",
      "epoch 61 / 100 loss: 1.939514 train acc: 0.585000; val_acc: 0.517778\n",
      "epoch 62 / 100 loss: 1.751419 train acc: 0.560000; val_acc: 0.535556\n",
      "epoch 63 / 100 loss: 1.873607 train acc: 0.556000; val_acc: 0.535556\n",
      "epoch 64 / 100 loss: 1.870228 train acc: 0.577000; val_acc: 0.551111\n",
      "epoch 65 / 100 loss: 1.853740 train acc: 0.571000; val_acc: 0.551111\n",
      "epoch 66 / 100 loss: 1.840162 train acc: 0.562000; val_acc: 0.560000\n",
      "epoch 67 / 100 loss: 1.897690 train acc: 0.593000; val_acc: 0.560000\n",
      "epoch 68 / 100 loss: 1.820656 train acc: 0.582000; val_acc: 0.564444\n",
      "epoch 69 / 100 loss: 1.813729 train acc: 0.593000; val_acc: 0.575556\n",
      "epoch 70 / 100 loss: 1.898763 train acc: 0.617000; val_acc: 0.564444\n",
      "epoch 71 / 100 loss: 1.711419 train acc: 0.599000; val_acc: 0.575556\n",
      "epoch 72 / 100 loss: 1.719922 train acc: 0.622000; val_acc: 0.575556\n",
      "epoch 73 / 100 loss: 1.787692 train acc: 0.643000; val_acc: 0.591111\n",
      "epoch 74 / 100 loss: 1.792937 train acc: 0.608000; val_acc: 0.593333\n",
      "epoch 75 / 100 loss: 1.876856 train acc: 0.613000; val_acc: 0.588889\n",
      "epoch 76 / 100 loss: 1.723642 train acc: 0.601000; val_acc: 0.595556\n",
      "epoch 77 / 100 loss: 1.716431 train acc: 0.632000; val_acc: 0.593333\n",
      "epoch 78 / 100 loss: 1.599544 train acc: 0.637000; val_acc: 0.597778\n",
      "epoch 79 / 100 loss: 1.694495 train acc: 0.635000; val_acc: 0.595556\n",
      "epoch 80 / 100 loss: 1.750291 train acc: 0.652000; val_acc: 0.600000\n",
      "epoch 81 / 100 loss: 1.629177 train acc: 0.631000; val_acc: 0.600000\n",
      "epoch 82 / 100 loss: 1.785082 train acc: 0.632000; val_acc: 0.597778\n",
      "epoch 83 / 100 loss: 1.625472 train acc: 0.651000; val_acc: 0.613333\n",
      "epoch 84 / 100 loss: 1.481268 train acc: 0.642000; val_acc: 0.615556\n",
      "epoch 85 / 100 loss: 1.594534 train acc: 0.674000; val_acc: 0.600000\n",
      "epoch 86 / 100 loss: 1.680630 train acc: 0.657000; val_acc: 0.620000\n",
      "epoch 87 / 100 loss: 1.537964 train acc: 0.641000; val_acc: 0.613333\n",
      "epoch 88 / 100 loss: 1.458538 train acc: 0.675000; val_acc: 0.646667\n",
      "epoch 89 / 100 loss: 1.564080 train acc: 0.667000; val_acc: 0.651111\n",
      "epoch 90 / 100 loss: 1.587606 train acc: 0.685000; val_acc: 0.646667\n",
      "epoch 91 / 100 loss: 1.623677 train acc: 0.704000; val_acc: 0.668889\n",
      "epoch 92 / 100 loss: 1.355936 train acc: 0.700000; val_acc: 0.666667\n",
      "epoch 93 / 100 loss: 1.512469 train acc: 0.720000; val_acc: 0.680000\n",
      "epoch 94 / 100 loss: 1.553492 train acc: 0.733000; val_acc: 0.675556\n",
      "epoch 95 / 100 loss: 1.478928 train acc: 0.708000; val_acc: 0.675556\n",
      "epoch 96 / 100 loss: 1.526060 train acc: 0.705000; val_acc: 0.662222\n",
      "epoch 97 / 100 loss: 1.381132 train acc: 0.715000; val_acc: 0.673333\n",
      "epoch 98 / 100 loss: 1.422049 train acc: 0.701000; val_acc: 0.675556\n",
      "epoch 99 / 100 loss: 1.353178 train acc: 0.729000; val_acc: 0.677778\n",
      "epoch 100 / 100 loss: 1.491319 train acc: 0.748000; val_acc: 0.688889\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = FullyConnected([100, 100, 100, 100],weight_scale=3e-2, dtype=np.float64)\n",
    "model.compile(\n",
    "              update_rule='sgd',\n",
    "              optim_config={\n",
    "                  'learning_rate': 1e-4,\n",
    "              }\n",
    "         )\n",
    "model.fit(X_train, y_train,X_test,y_test,epochs=100, batch_size=32)\n",
    "plt.plot(model.val_acc_history, 'o')\n",
    "plt.title('val_acc_history history')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('val_acc_history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到降低学习率后明显训练过程被拉长了，说明上一个训练是更合适的学习率。这次可视化做的有些残念，时间不够了以后再完善"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
