{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先过一下理论，随后整理代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What does a neuron compute?\n",
    "Within an artificial neural network, a neuron is a mathematical function that model the functioning of a biological neuron. Typically, a neuron compute the weighted average of its input, and this sum is passed through a nonlinear function, often called activation function.\n",
    "\n",
    "A hidden layer maps input features to  new features,which are computed by neurons that one neuron output/represent one new feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2. Why we use non-linear activation funcitons in neural networks?\n",
    "Non-linear functions address the problems of a linear activation function: They allow backpropagation because they have a derivative function which is related to the inputs. They allow “stacking” of multiple layers of neurons to create a deep neural network.\n",
    "\n",
    "Without non-linear activation funciton,no matter how deep the network is,it's the same as one layer on account of “stacking”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the 'Logistic Loss' ?\n",
    "\n",
    " The loss function of logistic regression is doing this exactly which is called Logistic Loss.\n",
    " \n",
    "$$L(\\hat{y} ,y) = -y log(\\hat{y})-(1-y)log(1-\\hat{y})$$\n",
    "$\\hat{y}$ is the predicted value and y is the label value.\n",
    "Intuitively, we want to assign more punishment when predicting 1 while the actual is 0 and when predict 0 while the actual is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Assume that you are building a binary classifier for detecting if an image containing cats, which activation functions would you recommen using for the output layer ?\n",
    "\n",
    "A. ReLU    \n",
    "\n",
    "B. Leaky ReLU  \n",
    "\n",
    "C. sigmoid         ✔ \n",
    "\n",
    "D. tanh  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Why we don't use zero initialization for all parameters ?\n",
    "If you initialize all weights with zeros,then every hidden unit will get zero independent of the input. So, when all the hidden neurons start with the zero weights, then all of them will follow the same gradient and for this reason \"it affects only the scale of the weight vector, not the direction\".\n",
    "\n",
    "Initializing all weights with a same value doesn't work,either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you implement the softmax function using python ? \n",
    "Sure,let's go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 先看下我们使用到的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1797个样本，每个样本8x8的像素\n",
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#这里存的是样本的label\n",
    "digits.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(1,11):\n",
    "    plt.subplot(2,5,i)\n",
    "    #数据集前9个数据正好是1~9\n",
    "    plt.imshow(digits.data[i-1].reshape([8,8]),cmap=plt.cm.gray_r)\n",
    "    #（3，10）为文本位置的坐标。每个框的左上角是（0，0），下边框是8因为像素是8x8\n",
    "    plt.text(3,10,str(digits.target[i-1]))\n",
    "    #关闭坐标轴刻度\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1347, 64)\n",
      "(450, 64)\n",
      "(1347,)\n",
      "(450,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.25)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 接下来是几种运算的正向传播和反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear/affine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正向"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$ z = w^T * x +b $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_forward(x, w, b):\n",
    "  \"\"\"\n",
    "  输入：\n",
    "  x的维度为(N, d_1, ..., d_k)。N是batchsize，每个样本有k个特征。我们可以将其reshape为（N，D）\n",
    "  w的维度为(D, M)，M为下一层神经元的个数(对应输出维度)\n",
    "  b的维度为(M,)\n",
    "  输出：\n",
    "  out的维度为(N, M)，作为下一层的输入\n",
    "  cache元组(x, w, b)存下反向传播中可以直接导入计算梯度\n",
    "  \"\"\"\n",
    "  out = None\n",
    "  N, D = x.shape[0], x.size / x.shape[0]\n",
    "  out = np.dot(x.reshape(N, D), w) + b\n",
    "  cache = (x, w, b)\n",
    "  return out, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 反向"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial{dz}}{\\partial{dx}} =dz*w^T$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial{dz}}{\\partial{dw}} =x^T*dz$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial{dz}}{\\partial{db}} =\\sum_{i=1}^{N}{dz(i,:)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后这个偏导写的pythonic了，一时想不到这个reshape数学上应该怎么写= =懂意思就行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_backward(dz, cache):\n",
    "  \"\"\"\n",
    "  输入:\n",
    "  dz是上一层返回的梯度，这里维度视为(N, M)\n",
    "  cache元组(x, w, b)为正向传播中存下的参数\n",
    "  输出:\n",
    "  输出为dz关于x，w，b的偏导dx，dw，db\n",
    "  dx的维度为(N, d_1, ..., d_k)\n",
    "  dw的维度为(D, M)\n",
    "  db的维度为(M,)\n",
    "  \"\"\"\n",
    "  x, w, b = cache\n",
    "  dx, dw, db = None, None, None\n",
    "  pass\n",
    "  N, D = x.shape[0], w.shape[0]\n",
    "  dx = np.dot(dz, w.T).reshape(x.shape)\n",
    "  dw = np.dot(x.reshape(N, D).T, dz)\n",
    "  db = np.sum(dz, axis = 0)\n",
    "\n",
    "  return dx, dw, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里就直接RELU吧，不用sigmoid了。下一个就是softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正向"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$ out = RELU(z)=max(0,z)$$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_forward(x):\n",
    "  \"\"\"\n",
    "  relu就是取正数部分不用多解释啦\n",
    "  \"\"\"\n",
    "  out = None\n",
    "  out = np.maximum(0, x)\n",
    "  cache = x\n",
    "  return out, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 反向"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial{dout}}{\\partial{dx}} =dout*bool(x>0)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  输入:\n",
    "  dout:上一步反向传播回来的梯度\n",
    "  cache:正向传播时记录下的参数\n",
    "  输出:\n",
    "  dout对于d的偏导\n",
    "  \"\"\"\n",
    "  dx, x = None, cache\n",
    "\n",
    "  dx = dout * (x > 0)\n",
    "\n",
    "  return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正向+反向"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为这一步是传播的终点，所以我们可以把正向和反向写在一个函数里"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$S(a_i) = \\frac{e^{a_i}}{\\sum_{j=1}^C e^{{a_j}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$l=-\\sum_{i=1}^{C}y^{(i)}log(S(a_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L=\\frac{1}{N}\\sum_{i=1}^{N}l$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了便于公式书写(否则又要写pythonic了)以上的S(a)是(1,C)的向量，y是0或1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dx反向传播求解步骤有点长这里我就不贴了，网上softmax反向传播能搜到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(x, y):\n",
    "  \"\"\"\n",
    "  输入:\n",
    "  x的维度为(N, C)，对应N个样本C个分类。x[i, j]代表第i个样本在第j个分类的得分\n",
    "  y的维度为(N,)，代表样本的label，为0~C-1的整数\n",
    "  输出:\n",
    "  loss是一个标量对应上面的L\n",
    "  dx是loss对于x的偏导，维度(N, C)。\n",
    "   \n",
    "  \"\"\"\n",
    "  #x - np.max是为了防止数值上溢\n",
    "  S = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "  #逐行对当前行求和(单个样本softmax)\n",
    "  S /= np.sum(S, axis=1, keepdims=True)\n",
    "  N = x.shape[0]\n",
    "  #逐列对当前列求和(所有样本的损失累加)，对于每个样本每一行只有对应y列(label)的log值才会被计入损失\n",
    "  loss = -np.sum(np.log(S[np.arange(N), y])) / N\n",
    "  dx = S.copy()\n",
    "  dx[np.arange(N), y] -= 1\n",
    "  dx /= N\n",
    "  return loss, dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优化器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里先只做一个SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\theta = \\theta - \\alpha*d\\theta$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(w, dw, config=None):\n",
    "  \"\"\"\n",
    "  输入:\n",
    "  w就是对应公式中的θ要优化的参数，dw是当前层返回梯度，config是一个包含所有超参数的字典\n",
    "  输出:\n",
    "  更新过后的w\n",
    "  \"\"\"\n",
    "  if config is None: config = {}\n",
    "  config.setdefault('learning_rate', 1e-2)\n",
    "  w -= config['learning_rate'] * dw\n",
    "  return w, config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 封装模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到这里时可以思考下之后要封装的模型了。不如我们模仿Keras，按照其提供的接口来封装模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仿照Keras，model实例化的时候传入模型的结构，model.compile传入损失函数、优化器、学习率，model.fit传入数据集完成训练过程同时记录损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected(object):\n",
    "    def __init__(self, hidden_dims, input_dim=8*8, num_classes=10,\n",
    "               reg=0.0, weight_scale=1e-2, dtype=np.float32):\n",
    "    \"\"\"\n",
    "    hidden_dims为列表表示模型的结构，如[3,2,1]即代表第一层隐藏层3个神经元，第二层隐藏层2个神经元，第三层隐藏层1个神经元\n",
    "    input_dim输入维度，默认对应mnist输入\n",
    "    num_classes等同于输出层神经元个数，默认对应mnist类别\n",
    "    reg为正则化系数\n",
    "    weight_scale为系数的初始化\n",
    "    dtype=np.float32代表所有系数精度为float32(也可以设为float64)\n",
    "\n",
    "    \"\"\"\n",
    "    self.reg = reg\n",
    "    #总层数算上输出层\n",
    "    self.num_layers = 1 + len(hidden_dims)\n",
    "    self.dtype = dtype\n",
    "    #params字典用于存储系数\n",
    "    self.params = {}\n",
    "    #初始化第一层系数，这里W按照正态分布\n",
    "    self.params['W1'] = np.random.randn(input_dim, hidden_dims[0]) * weight_scale\n",
    "    self.params['b1'] = np.zeros(hidden_dims[0])\n",
    "    #初始化第二层开始的系数\n",
    "    for i in range(self.num_layers - 2):\n",
    "        self.params['W' + str(i+2)] = np.random.randn(hidden_dims[i], hidden_dims[i+1]) * weight_scale\n",
    "        self.params['b' + str(i+2)] = np.zeros(hidden_dims[i+1])\n",
    "    #初始化最后一层的系数\n",
    "    self.params['W' + str(self.num_layers)] = np.random.randn(hidden_dims[-1], num_classes)\n",
    "    self.params['b' + str(self.num_layers)] = np.random.randn(num_classes)\n",
    "    #确定系数精度\n",
    "    for k, v in self.params.iteritems():\n",
    "        self.params[k] = v.astype(dtype)\n",
    "\n",
    "    def loss(self, x, y=None):\n",
    "    \"\"\"\n",
    "    输入：\n",
    "    x的维度为(N, d_1, ..., d_k)。N是batchsize，每个样本有k个特征。我们可以将其reshape为（N，D）\n",
    "    y的维度为(N,)，代表样本的label，y也可以为None\n",
    "    输出:\n",
    "    当y不为None时，说明这是训练的过程，返回loss，梯度\n",
    "    当y为None的时候，说明这是测试的过程，返回score\n",
    "    \"\"\"\n",
    "    x = x.astype(self.dtype)\n",
    "    mode = 'test' if y is None else 'train'\n",
    "    scores = None\n",
    "    #初始化两个列表，用来存每一层的输出和系数\n",
    "    hidden_layers, caches = range(self.num_layers + 1), range(self.num_layers)\n",
    "    hidden_layers[0] = x\n",
    "    for i in range(self.num_layers):\n",
    "        W, b = self.params['W' + str(i+1)], self.params['b' + str(i+1)]\n",
    "        #最后一层没有RELU\n",
    "        if i == self.num_layers - 1:\n",
    "            hidden_layers[i+1], caches[i] = affine_forward(hidden_layers[i], W, b)\n",
    "        else:\n",
    "            a, fc_cache = affine_forward(x, w, b)\n",
    "            out, relu_cache = relu_forward(a)\n",
    "            cache = (fc_cache, relu_cache)\n",
    "            hidden_layers[i+1], caches[i] = out, cache\n",
    "    #这里的score是没经过softmax的\n",
    "    scores = hidden_layers[self.num_layers]   \n",
    "    \n",
    "    if mode == 'test':\n",
    "    return scores\n",
    "\n",
    "    loss, grads = 0.0, {}\n",
    "    loss, dscores = softmax_loss(scores, y)\n",
    "    #初始化列表存储每一层梯度\n",
    "    dhiddens = range(self.num_layers + 1)\n",
    "    dhiddens[self.num_layers] = dscores\n",
    "    for i in range(self.num_layers, 0, -1):\n",
    "        #最后一层没有RELU\n",
    "        if i == self.num_layers:\n",
    "            dhiddens[i-1], grads['W' + str(i)], grads['b'+str(i)] = affine_backward(dhiddens[i], caches[i-1])\n",
    "        else:\n",
    "            dx, dw, db = affine_relu_backward(dhiddens[i], caches[i-1])\n",
    "            dhiddens[i-1], grads['W' + str(i)], grads['b' + str(i)] = dx, dw, db\n",
    "        #L2正则化\n",
    "        loss += 0.5 * self.reg * np.sum(self.params['W' + str(i)] ** 2)\n",
    "        grads['W' + str(i)] += self.reg * self.params['W' + str(i)]\n",
    "\n",
    "    return loss, grads"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
